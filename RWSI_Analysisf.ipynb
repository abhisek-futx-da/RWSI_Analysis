{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSw0NddicyRR"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check current directory and files\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"Files in directory:\", os.listdir('.'))\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo_0CKLycyRU"
      },
      "source": [
        "## 1. Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "841AFlQacyRV"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('rwsi_data.csv')\n",
        "\n",
        "\n",
        "print(\"RETAIL WEB SESSION INTELLIGENCE (RWSI) DATA ANALYSIS\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Total sessions: {len(df)}\")\n",
        "print(f\"Features: {len(df.columns)}\")\n",
        "\n",
        "\n",
        "print(f\"\\nColumn names:\")\n",
        "print(list(df.columns))\n",
        "\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes.value_counts().to_dict())\n",
        "\n",
        "print(f\"\\nTarget variable distribution:\")\n",
        "print(df['MonetaryConversion'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd8Zdm1qcyRW"
      },
      "outputs": [],
      "source": [
        "# Test data loading\n",
        "print(\"Testing data loading...\")\n",
        "print(f\"DataFrame type: {type(df)}\")\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikc0ogrRcyRW"
      },
      "outputs": [],
      "source": [
        "# Verify data loading worked\n",
        "\n",
        "print(f\" DataFrame created: {type(df)}\")\n",
        "print(f\" Shape: {df.shape}\")\n",
        "print(f\" Columns: {len(df.columns)}\")\n",
        "print(f\" Target variable unique values: {df['MonetaryConversion'].unique()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzPsKkRpcyRY"
      },
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percent = (missing_data / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_data,\n",
        "    'Missing Percentage': missing_percent\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0tM6BjVcyRY"
      },
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QlUfgMdcyRZ"
      },
      "outputs": [],
      "source": [
        "# Target variable analysis\n",
        "\n",
        "conversion_counts = df['MonetaryConversion'].value_counts()\n",
        "print(conversion_counts)\n",
        "conversion_rate = (df['MonetaryConversion'] == 'Yes').mean()\n",
        "print(f\"Overall conversion rate: {conversion_rate:.2%}\")\n",
        "\n",
        "# Behavior differences between converting and non-converting sessions\n",
        "print(\"\\nBehavior differences between converting and non-converting sessions:\")\n",
        "converting_sessions = df[df['MonetaryConversion'] == 'Yes']\n",
        "non_converting_sessions = df[df['MonetaryConversion'] == 'No']\n",
        "\n",
        "print(\"\\nAverage metrics comparison:\")\n",
        "comparison_metrics = ['AdClicks', 'ItemBrowseCount', 'ItemBrowseTime', 'PageEngagementScore']\n",
        "for metric in comparison_metrics:\n",
        "    if metric in df.columns:\n",
        "        conv_avg = converting_sessions[metric].mean()\n",
        "        non_conv_avg = non_converting_sessions[metric].mean()\n",
        "        print(f\"{metric}: Converting={conv_avg:.2f}, Non-converting={non_conv_avg:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgpCbMmUcyRa"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive EDA visualizations\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Plot 1: Target distribution\n",
        "plt.subplot(3, 4, 1)\n",
        "df['MonetaryConversion'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "plt.title('Conversion Distribution')\n",
        "\n",
        "# Plot 2: AdClicks by conversion\n",
        "plt.subplot(3, 4, 2)\n",
        "df.boxplot(column='AdClicks', by='MonetaryConversion', ax=plt.gca())\n",
        "plt.title('Ad Clicks by Conversion')\n",
        "plt.suptitle('')  # Remove default title\n",
        "\n",
        "# Plot 3: Item Browse Count by conversion\n",
        "plt.subplot(3, 4, 3)\n",
        "df.boxplot(column='ItemBrowseCount', by='MonetaryConversion', ax=plt.gca())\n",
        "plt.title('Item Browse Count by Conversion')\n",
        "plt.suptitle('')\n",
        "\n",
        "# Plot 4: Platform distribution\n",
        "plt.subplot(3, 4, 4)\n",
        "df['UserPlatformID'].value_counts().head().plot(kind='bar')\n",
        "plt.title('User Platform Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 5: Market Zone distribution\n",
        "plt.subplot(3, 4, 5)\n",
        "df['MarketZone'].value_counts().head().plot(kind='bar')\n",
        "plt.title('Market Zone Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 6: User Category distribution\n",
        "plt.subplot(3, 4, 6)\n",
        "df['UserCategory'].value_counts().plot(kind='bar')\n",
        "plt.title('User Category Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 7: Weekend vs Weekday conversion\n",
        "plt.subplot(3, 4, 7)\n",
        "weekend_conv = df.groupby('IsWeekendVisit')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean())\n",
        "weekend_conv.plot(kind='bar')\n",
        "plt.title('Conversion Rate: Weekend vs Weekday')\n",
        "plt.ylabel('Conversion Rate')\n",
        "plt.xticks([0, 1], ['Weekday', 'Weekend'], rotation=0)\n",
        "\n",
        "# Plot 8: Traffic Source analysis\n",
        "plt.subplot(3, 4, 8)\n",
        "traffic_conv = df.groupby('TrafficSourceCode')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "traffic_conv.head().plot(kind='bar')\n",
        "plt.title('Conversion Rate by Traffic Source')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 9: Monthly trends\n",
        "plt.subplot(3, 4, 9)\n",
        "monthly_conv = df.groupby('VisitMonth')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean())\n",
        "monthly_conv.plot(kind='line', marker='o')\n",
        "plt.title('Monthly Conversion Trends')\n",
        "plt.ylabel('Conversion Rate')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 10: Platform conversion rates\n",
        "plt.subplot(3, 4, 10)\n",
        "platform_conv = df.groupby('UserPlatformID')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "platform_conv.head().plot(kind='bar')\n",
        "plt.title('Conversion Rate by Platform')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 11: Market Zone conversion rates\n",
        "plt.subplot(3, 4, 11)\n",
        "market_conv = df.groupby('MarketZone')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "market_conv.head().plot(kind='bar')\n",
        "plt.title('Conversion Rate by Market Zone')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 12: User Category conversion rates\n",
        "plt.subplot(3, 4, 12)\n",
        "user_conv = df.groupby('UserCategory')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "user_conv.plot(kind='bar')\n",
        "plt.title('Conversion Rate by User Category')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rwsi_comprehensive_eda.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RLEs_IgcyRa"
      },
      "source": [
        "## 3. Missing Value Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOUUnD0TcyRb"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "# Create a copy for processing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Fill missing values with median for numerical columns\n",
        "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numerical_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        median_val = df_processed[col].median()\n",
        "        df_processed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"  {col}: Filled with median {median_val:.2f}\")\n",
        "\n",
        "# Fill missing values with mode for categorical columns\n",
        "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if col != 'SessionID' and df_processed[col].isnull().sum() > 0:\n",
        "        mode_val = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'\n",
        "        df_processed[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"  {col}: Filled with mode '{mode_val}'\")\n",
        "\n",
        "print(f\"\\nRemaining missing values: {df_processed.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ76HUYbcyRb"
      },
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im7yfQAxcyRb"
      },
      "outputs": [],
      "source": [
        "# Create new features\n",
        "print(\"Creating new features...\")\n",
        "\n",
        "# Session engagement score\n",
        "df_processed['SessionEngagementScore'] = (\n",
        "    df_processed['AdClicks'] * 0.3 +\n",
        "    df_processed['InfoSectionCount'] * 0.2 +\n",
        "    df_processed['ItemBrowseCount'] * 0.5\n",
        ")\n",
        "\n",
        "# Time efficiency score\n",
        "df_processed['TimeEfficiencyScore'] = (\n",
        "    df_processed['ItemBrowseCount'] / (df_processed['ItemBrowseTime'] + 1)\n",
        ")\n",
        "\n",
        "# Help seeking behavior\n",
        "df_processed['HelpSeekingBehavior'] = (\n",
        "    df_processed['HelpPageVisits'] + df_processed['InfoSectionCount']\n",
        ")\n",
        "\n",
        "# High-value session indicator\n",
        "df_processed['HighValueSession'] = (\n",
        "    (df_processed['AdClicks'] > df_processed['AdClicks'].quantile(0.75)) &\n",
        "    (df_processed['ItemBrowseCount'] > df_processed['ItemBrowseCount'].quantile(0.75))\n",
        ").astype(int)\n",
        "\n",
        "# Session duration category\n",
        "df_processed['SessionDurationCategory'] = pd.cut(\n",
        "    df_processed['ItemBrowseTime'],\n",
        "    bins=[0, 100, 500, 1000, float('inf')],\n",
        "    labels=['Very Short', 'Short', 'Medium', 'Long']\n",
        ")\n",
        "\n",
        "# Platform-Browser interaction\n",
        "df_processed['PlatformBrowser'] = (\n",
        "    df_processed['UserPlatformID'].astype(str) + '_' +\n",
        "    df_processed['WebClientCode'].astype(str)\n",
        ")\n",
        "\n",
        "print(\"New features created:\")\n",
        "new_features = ['SessionEngagementScore', 'TimeEfficiencyScore', 'HelpSeekingBehavior',\n",
        "               'HighValueSession', 'SessionDurationCategory', 'PlatformBrowser']\n",
        "for feature in new_features:\n",
        "    print(f\"   {feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAjYSH_IcyRb"
      },
      "source": [
        "## 5. Model Training - 5 Different Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSl-TRbFcyRb"
      },
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "print(\"Preparing data for modeling...\")\n",
        "\n",
        "# Encode categorical variables\n",
        "le_visit_month = LabelEncoder()\n",
        "le_platform = LabelEncoder()\n",
        "le_web_client = LabelEncoder()\n",
        "le_market_zone = LabelEncoder()\n",
        "le_user_category = LabelEncoder()\n",
        "le_platform_browser = LabelEncoder()\n",
        "le_duration_cat = LabelEncoder()\n",
        "\n",
        "df_processed['VisitMonth_encoded'] = le_visit_month.fit_transform(df_processed['VisitMonth'].astype(str))\n",
        "df_processed['UserPlatformID_encoded'] = le_platform.fit_transform(df_processed['UserPlatformID'].astype(str))\n",
        "df_processed['WebClientCode_encoded'] = le_web_client.fit_transform(df_processed['WebClientCode'].astype(str))\n",
        "df_processed['MarketZone_encoded'] = le_market_zone.fit_transform(df_processed['MarketZone'].astype(str))\n",
        "df_processed['UserCategory_encoded'] = le_user_category.fit_transform(df_processed['UserCategory'].astype(str))\n",
        "df_processed['PlatformBrowser_encoded'] = le_platform_browser.fit_transform(df_processed['PlatformBrowser'].astype(str))\n",
        "df_processed['SessionDurationCategory_encoded'] = le_duration_cat.fit_transform(df_processed['SessionDurationCategory'].astype(str))\n",
        "\n",
        "# Select features for modeling\n",
        "feature_columns = [\n",
        "    'AdClicks', 'InfoSectionCount', 'InfoSectionTime', 'HelpPageVisits',\n",
        "    'HelpPageTime', 'ItemBrowseCount', 'ItemBrowseTime', 'ExitRateFirstPage',\n",
        "    'SessionExitRatio', 'PageEngagementScore', 'HolidayProximityIndex',\n",
        "    'TrafficSourceCode', 'IsWeekendVisit', 'VisitMonth_encoded',\n",
        "    'UserPlatformID_encoded', 'WebClientCode_encoded', 'MarketZone_encoded',\n",
        "    'UserCategory_encoded', 'SessionEngagementScore', 'TimeEfficiencyScore',\n",
        "    'HelpSeekingBehavior', 'HighValueSession', 'PlatformBrowser_encoded',\n",
        "    'SessionDurationCategory_encoded'\n",
        "]\n",
        "\n",
        "X = df_processed[feature_columns]\n",
        "y = (df_processed['MonetaryConversion'] == 'Yes').astype(int)\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHAY6W8ZcyRc"
      },
      "outputs": [],
      "source": [
        "# Train all 5 models\n",
        "# Scale features for models that need it\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
        "print(f\"  Accuracy: {lr_accuracy:.4f}, AUC: {lr_auc:.4f}\")\n",
        "\n",
        "# 2. Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
        "print(f\"  Accuracy: {rf_accuracy:.4f}, AUC: {rf_auc:.4f}\")\n",
        "\n",
        "# 3. Linear Regression\n",
        "print(\"Training Linear Regression...\")\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "linear_pred_proba = linear_model.predict(X_test_scaled)\n",
        "linear_pred = (linear_pred_proba > 0.5).astype(int)\n",
        "linear_accuracy = accuracy_score(y_test, linear_pred)\n",
        "linear_auc = roc_auc_score(y_test, linear_pred_proba)\n",
        "linear_mse = mean_squared_error(y_test, linear_pred_proba)\n",
        "linear_r2 = r2_score(y_test, linear_pred_proba)\n",
        "print(f\"  Accuracy: {linear_accuracy:.4f}, AUC: {linear_auc:.4f}, MSE: {linear_mse:.4f}, R²: {linear_r2:.4f}\")\n",
        "\n",
        "# 4. Gradient Descent (SGD Classifier)\n",
        "print(\"Training Gradient Descent (SGD Classifier)...\")\n",
        "sgd_model = SGDClassifier(random_state=42, max_iter=1000, loss='log_loss')\n",
        "sgd_model.fit(X_train_scaled, y_train)\n",
        "sgd_pred = sgd_model.predict(X_test_scaled)\n",
        "sgd_pred_proba = sgd_model.predict_proba(X_test_scaled)[:, 1]\n",
        "sgd_accuracy = accuracy_score(y_test, sgd_pred)\n",
        "sgd_auc = roc_auc_score(y_test, sgd_pred_proba)\n",
        "print(f\"  Accuracy: {sgd_accuracy:.4f}, AUC: {sgd_auc:.4f}\")\n",
        "\n",
        "# 5. Decision Tree\n",
        "print(\"Training Decision Tree...\")\n",
        "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "dt_auc = roc_auc_score(y_test, dt_pred_proba)\n",
        "print(f\"  Accuracy: {dt_accuracy:.4f}, AUC: {dt_auc:.4f}\")\n",
        "\n",
        "print(\"\\nAll models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUZd-wsScyRc"
      },
      "source": [
        "## 6. Model Evaluation and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW4QJDHGcyRc"
      },
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "models = {\n",
        "    'Logistic Regression': (lr_model, lr_pred, lr_accuracy, lr_auc, 'scaled'),\n",
        "    'Random Forest': (rf_model, rf_pred, rf_accuracy, rf_auc, 'unscaled'),\n",
        "    'Linear Regression': (linear_model, linear_pred, linear_accuracy, linear_auc, 'scaled'),\n",
        "    'Gradient Descent': (sgd_model, sgd_pred, sgd_accuracy, sgd_auc, 'scaled'),\n",
        "    'Decision Tree': (dt_model, dt_pred, dt_accuracy, dt_auc, 'unscaled')\n",
        "}\n",
        "\n",
        "# Create comprehensive comparison table\n",
        "print(f\"{'Model':<20} {'Accuracy':<10} {'AUC':<10} {'MSE':<10} {'R²':<10} {'Data Type':<12}\")\n",
        "\n",
        "\n",
        "for name, (model, pred, acc, auc, data_type) in models.items():\n",
        "    if name == 'Linear Regression':\n",
        "        mse = linear_mse\n",
        "        r2 = linear_r2\n",
        "    else:\n",
        "        mse = mean_squared_error(y_test, pred)\n",
        "        r2 = r2_score(y_test, pred)\n",
        "\n",
        "    print(f\"{name:<20} {acc:<10.4f} {auc:<10.4f} {mse:<10.4f} {r2:<10.4f} {data_type:<12}\")\n",
        "\n",
        "# Find best model by AUC\n",
        "best_model_name = max(models.keys(), key=lambda x: models[x][3])  # Best by AUC\n",
        "best_model, best_pred, best_accuracy, best_auc, best_data_type = models[best_model_name]\n",
        "\n",
        "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
        "print(f\"  Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"  AUC Score: {best_auc:.4f}\")\n",
        "print(f\"  Data Type: {best_data_type}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEjayXM4cyRc"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualization comparing all models\n",
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(2, 4, 1)\n",
        "model_names = list(models.keys())\n",
        "accuracies = [models[name][2] for name in model_names]\n",
        "colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']\n",
        "bars = plt.bar(model_names, accuracies, color=colors)\n",
        "plt.title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# AUC comparison\n",
        "plt.subplot(2, 4, 2)\n",
        "auc_scores = [models[name][3] for name in model_names]\n",
        "bars = plt.bar(model_names, auc_scores, color=colors)\n",
        "plt.title('Model AUC Score Comparison', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('AUC Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for i, v in enumerate(auc_scores):\n",
        "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Confusion matrix for best model\n",
        "plt.subplot(2, 4, 3)\n",
        "cm = confusion_matrix(y_test, best_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=['No Conversion', 'Conversion'],\n",
        "           yticklabels=['No Conversion', 'Conversion'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Model performance heatmap\n",
        "plt.subplot(2, 4, 4)\n",
        "comparison_data = []\n",
        "for name in model_names:\n",
        "    _, _, acc, auc, _ = models[name]\n",
        "    comparison_data.append([acc, auc])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data,\n",
        "                           index=model_names,\n",
        "                           columns=['Accuracy', 'AUC'])\n",
        "sns.heatmap(comparison_df, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Score'})\n",
        "plt.title('Model Performance Heatmap', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Models')\n",
        "plt.xlabel('Metrics')\n",
        "\n",
        "# Best model summary\n",
        "plt.subplot(2, 4, 5)\n",
        "plt.axis('off')\n",
        "summary_text = f\"\"\"\n",
        "BEST MODEL: {best_model_name}\n",
        "\n",
        "Performance Metrics:\n",
        "• Accuracy: {best_accuracy:.4f}\n",
        "• AUC Score: {best_auc:.4f}\n",
        "• Data Type: {best_data_type}\n",
        "\n",
        "Model Type: {'Classification' if best_model_name != 'Linear Regression' else 'Regression'}\n",
        "\n",
        "Key Insights:\n",
        "• Best overall performance\n",
        "• Suitable for production use\n",
        "• Handles feature scaling: {'Yes' if best_data_type == 'scaled' else 'No'}\n",
        "\"\"\"\n",
        "plt.text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comprehensive_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmA8AKokcyRd"
      },
      "source": [
        "## 7. Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TctcfmegcyRd"
      },
      "outputs": [],
      "source": [
        "# Feature importance for best model\n",
        "if best_model_name in ['Random Forest', 'Decision Tree']:\n",
        "    feature_importance = best_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"Top 15 Most Important Features for {best_model_name}:\")\n",
        "    print(importance_df.head(15))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = importance_df.head(15)\n",
        "    sns.barplot(data=top_features, x='importance', y='feature')\n",
        "    plt.title(f'Feature Importance - {best_model_name}')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
        "print(classification_report(y_test, best_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSmSINM6cyRd"
      },
      "source": [
        "## 8. Business Insights and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3He392aTcyRd"
      },
      "outputs": [],
      "source": [
        "# Generate business insights\n",
        "print(\"KEY FINDINGS AND INSIGHTS\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Behavioral patterns\n",
        "print(\"\\n1. BEHAVIORAL PATTERNS:\")\n",
        "print(f\"   - Converting sessions: {converting_sessions['ItemBrowseCount'].mean():.1f} avg item views\")\n",
        "print(f\"   - Non-converting sessions: {non_converting_sessions['ItemBrowseCount'].mean():.1f} avg item views\")\n",
        "print(f\"   - Converting sessions spend {converting_sessions['ItemBrowseTime'].mean():.1f} seconds browsing\")\n",
        "print(f\"   - Non-converting sessions spend {non_converting_sessions['ItemBrowseTime'].mean():.1f} seconds browsing\")\n",
        "\n",
        "# Platform insights\n",
        "print(\"\\n2. PLATFORM INSIGHTS:\")\n",
        "best_platform = df_processed.groupby('UserPlatformID')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "print(f\"   - Best platform: {best_platform.index[0]} ({best_platform.iloc[0]:.2%} conversion)\")\n",
        "\n",
        "# Geographic insights\n",
        "print(\"\\n3. GEOGRAPHIC INSIGHTS:\")\n",
        "best_market = df_processed.groupby('MarketZone')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "print(f\"   - Best market: {best_market.index[0]} ({best_market.iloc[0]:.2%} conversion)\")\n",
        "\n",
        "# User segmentation\n",
        "print(\"\\n4. USER SEGMENTATION:\")\n",
        "best_user = df_processed.groupby('UserCategory')['MonetaryConversion'].apply(lambda x: (x == 'Yes').mean()).sort_values(ascending=False)\n",
        "print(f\"   - Best user category: {best_user.index[0]} ({best_user.iloc[0]:.2%} conversion)\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRUnINUFcyRd"
      },
      "source": [
        "## 9. Hyperparameter Tuning\n",
        "\n",
        "Now we'll perform hyperparameter tuning for each model to optimize their performance using GridSearchCV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiHmDqUxcyRd"
      },
      "outputs": [],
      "source": [
        "# Import GridSearchCV for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "print(\"Starting hyperparameter tuning for all models...\")\n",
        "print(\"This may take several minutes to complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jklDCZJcyRe"
      },
      "source": [
        "### 9.2 Random Forest Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4cfFWYVcyRe"
      },
      "outputs": [],
      "source": [
        "# Random Forest Hyperparameter Tuning\n",
        "print(\"Tuning Random Forest hyperparameters...\")\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "rf_grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the grid search\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and score\n",
        "rf_best_params = rf_grid_search.best_params_\n",
        "rf_best_score = rf_grid_search.best_score_\n",
        "\n",
        "print(f\"Best parameters: {rf_best_params}\")\n",
        "print(f\"Best CV score: {rf_best_score:.4f}\")\n",
        "\n",
        "# Train model with best parameters\n",
        "rf_tuned = RandomForestClassifier(**rf_best_params, random_state=42)\n",
        "rf_tuned.fit(X_train, y_train)\n",
        "rf_tuned_pred = rf_tuned.predict(X_test)\n",
        "rf_tuned_pred_proba = rf_tuned.predict_proba(X_test)[:, 1]\n",
        "rf_tuned_accuracy = accuracy_score(y_test, rf_tuned_pred)\n",
        "rf_tuned_auc = roc_auc_score(y_test, rf_tuned_pred_proba)\n",
        "\n",
        "print(f\"Tuned Random Forest - Accuracy: {rf_tuned_accuracy:.4f}, AUC: {rf_tuned_auc:.4f}\")\n",
        "print(f\"Improvement: Accuracy {rf_tuned_accuracy - rf_accuracy:+.4f}, AUC {rf_tuned_auc - rf_auc:+.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}